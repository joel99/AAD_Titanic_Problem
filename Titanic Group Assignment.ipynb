{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:\n",
    "    Aaron McDaniel,\n",
    "    Jeffrey Minowa,\n",
    "    Joshua Reno,\n",
    "    & Joel Ye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all of the libraries and files we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import svm\n",
    "import rfClassifier\n",
    "import knnClassifier\n",
    "import gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need methods to import the data into a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    url = 'data/' + filename\n",
    "    df = pd.read_csv(url, sep=',')\n",
    "    print(\"Loaded \" + filename)\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will read in a csv file in the data folder and convert it into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_all():\n",
    "    le = LabelEncoder()\n",
    "    train_data = load_data(train_fn)\n",
    "    test_data = load_data(test_fn)\n",
    "    test_labels = load_data(test_label_fn)\n",
    "\n",
    "    # Note test data has different data order\n",
    "    # Convert sex column (col 4)\n",
    "    le.fit([\"male\", \"female\"])\n",
    "    train_data[:, 4] = le.transform(train_data[:, 4])\n",
    "    test_data[:, 3] = le.transform(test_data[:, 3])\n",
    "    # Convert embark column (col 11)\n",
    "    # le.fit([\"S\", \"C\", \"Q\", None])\n",
    "    # print(train_data[:, 11])\n",
    "    # train_data[:, 11] = le.transform(train_data[:, 11])\n",
    "    # test_data[:, 10] = le.transform(test_data[:, 10])\n",
    "    \n",
    "    # Feature selection:\n",
    "    # Trim passenger_id (c0), name (c3), ticket number (c8), cabin number (c10)\n",
    "    # As we're unsure about cabin_number domain effect, we're just dropping it\n",
    "    # Dropping embark since we think it's not too helpful, and has NaN\n",
    "    train_data = np.delete(train_data, [0, 3, 8, 10, 11], axis = 1)\n",
    "    test_data = np.delete(train_data, [0, 2, 7, 9, 10], axis = 1)\n",
    "    \n",
    "    # Fill in NaN\n",
    "    train_data = np.where(pd.isnull(train_data), -1, train_data)\n",
    "    x_test = np.where(pd.isnull(test_data), -1, test_data)\n",
    "    y_test = test_labels\n",
    "\n",
    "    # Separate train_data into x and y\n",
    "    x_train = train_data[:, 1:].astype('float')\n",
    "    y_train = train_data[:, 0].astype('int')\n",
    "    return ((x_train, y_train), (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes in all relevent files and creates train and test datasets that are further split up into data and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need methods that can evaluate and rank classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(clf, data, labels):\n",
    "    \"\"\"\n",
    "    calculates the precision and recall for the given classifier on the given set of data and labels\n",
    "\n",
    "    :param clf: untrained classifier to be evaluated\n",
    "    :param data: the dataset used for cross validation\n",
    "    :param labels: the correct labels that match with the given data\n",
    "    :return: a tuple of the precision and recall scores for the given classifier\n",
    "    \"\"\"\n",
    "\n",
    "    precision = cross_val_score(clf, data, labels, scoring='precision', cv=5, n_jobs=-1)\n",
    "    recall = cross_val_score(clf, data, labels, scoring='recall', cv=5, n_jobs=-1)\n",
    "    precision = precision.mean()\n",
    "    recall = recall.mean()\n",
    "\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above will return the average precision and recall\n",
    "of the inputted classifier obtained during 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_FP_FN(labels, precision, recall):\n",
    "    \"\"\"\n",
    "    converts form precision and recall to FP and FN.\n",
    "    Since Recall = TP/(TP + FN), TP = Recall * Positives\n",
    "    This means we can solve for FN & FP with\n",
    "    FN = TP/Recall - TP\n",
    "    FP = TP/Precision - TP\n",
    "\n",
    "    :param labels: the list of numeric labels that the precision and recall metrics came from\n",
    "    :param precision: the precision of some classifier on the given labels\n",
    "    :param recall: the recall of some classifier on the given labels\n",
    "    :return: a tuple containing FP and FN in that order\n",
    "    \"\"\"\n",
    "    positives = sum([1 for l in labels if l == 1])\n",
    "    tp = recall * positives\n",
    "    fn = tp / recall - tp\n",
    "    fp = tp / precision - tp\n",
    "\n",
    "    return (fp, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since score returns precision and recall, the above method converts\n",
    "those metrics to False positives and false negatives with added\n",
    "information from the dataset labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_dominance_min(ind1, ind2):\n",
    "    \"\"\"\n",
    "    returns true if ind1 dominates ind2 by the metrics that should be minimized\n",
    "\n",
    "    :param ind1: tuple of FP and FN\n",
    "    :param ind2: tuple of FP and FN\n",
    "    :return: boolean representing if ind1 dominates ind2 using the metrics that should be minimized\n",
    "    \"\"\"\n",
    "\n",
    "    not_equal = False\n",
    "    for value_1, value_2 in zip(ind1, ind2):\n",
    "        if value_1 < value_2:\n",
    "            return False\n",
    "        elif value_1 < value_2:\n",
    "            not_equal = True\n",
    "    return not_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above will return a boolean representation of if ind1 \n",
    "is pareto dominant compared to ind2 assuming that the 2 scores \n",
    "assiciated with each individual should be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_front(front, ind, comp):\n",
    "    \"\"\"\n",
    "    Makes a new pareto front out of the old pareto front and new individual\n",
    "    In this context an individual consists of scores and their hyper parameters\n",
    "    For example ind[0] is a tuple of precision and recall scores\n",
    "    and ind[1] is a list of the hyper-parameters needed to recreate the classifier\n",
    "\n",
    "    :param front: the old pareto front to be updated\n",
    "    :param ind: the new individual that may or may not change the old pareto front\n",
    "    :param comp: the method used to compare individuals as being pareto dominant or not\n",
    "    :return: the new pareto front\n",
    "    \"\"\"\n",
    "\n",
    "    front.append(ind)\n",
    "    all = front\n",
    "\n",
    "    front = []\n",
    "    for ind1 in all:\n",
    "        pareto = True\n",
    "        for ind2 in front:\n",
    "            if comp(ind2[0],ind1[0]):\n",
    "                # ind1 cannot belong on the pareto front\n",
    "                pareto = False\n",
    "                break\n",
    "            elif comp(ind1[0],ind2[0]):\n",
    "                #ind1 belongs on pareto front and ind2 does not\n",
    "                front.remove(ind2)\n",
    "        if pareto:\n",
    "            front += [ind1]\n",
    "    return front"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method will return the pareto front consisting of the \n",
    "old pareto front and one new individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
